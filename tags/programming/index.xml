<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Programming on Jill Munson</title>
    <link>http://localhost:1313/tags/programming/</link>
    <description>Recent content in Programming on Jill Munson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Jul 2015 12:57:25 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/programming/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>An Introduction to LRU Caches</title>
      <link>http://localhost:1313/post/2015/7-17/lru-cache/</link>
      <pubDate>Fri, 17 Jul 2015 12:57:25 -0400</pubDate>
      
      <guid>http://localhost:1313/post/2015/7-17/lru-cache/</guid>
      <description>

&lt;p&gt;When building and subsequently scaling a website, caching is important.  At the scale of hundreds of thousands of users a day, having multiple layers of caching becomes critical.&lt;/p&gt;

&lt;p&gt;On the job, we use the in-memory data store &lt;a href=&#34;http://redis.io/&#34;&gt;redis&lt;/a&gt; heavily (someone did a talk about it once which you can find &lt;a href=&#34;http://blog.pivotal.io/pivotal/case-studies-2/8-ways-media-giant-viacom-uses-redis-to-serve-dynamic-video-at-scale&#34;&gt;here&lt;/a&gt;).  At a high level, we use it to support data coming into our sites so when we do things on the site that require lots of rapidly-changing content it&amp;rsquo;s pretty important to bust cache while making sure the site stays up.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve also done two technical interviews in which I&amp;rsquo;ve been asked to implement a cache with a &lt;a href=&#34;https://en.wikipedia.org/wiki/Page_replacement_algorithm#Least_recently_used&#34;&gt;least recently used&lt;/a&gt; eviction policy.  In a nutshell, LRU is a policy of dropping the &amp;ldquo;least recently used&amp;rdquo; item from cache to make room for a most recently used item that does not exist in the cache.&lt;/p&gt;

&lt;p&gt;Question: Implement a cache with an LRU policy that has O(1) insertion, O(1) lookup, and O(1) removal.&lt;/p&gt;

&lt;h4 id=&#34;insertion:5fb571edf0462b9520231a3fff3fe887&#34;&gt;Insertion&lt;/h4&gt;

&lt;p&gt;In order to achieve O(1) insertion, you will need to store items themselves in a hash map or a linked list.  At first, it seems obvious that this list should be ordered so a queue might suffice.&lt;/p&gt;

&lt;h4 id=&#34;lookup:5fb571edf0462b9520231a3fff3fe887&#34;&gt;Lookup&lt;/h4&gt;

&lt;p&gt;Since lookup needs to be O(1), we need to store items in a data structure that gives us an O(1) key-value mapping.  However, we also need to be aware of the order of insertion &lt;em&gt;and&lt;/em&gt; order of being looked at, which we lose if we get rid of the queue entirely.&lt;/p&gt;

&lt;h4 id=&#34;removal:5fb571edf0462b9520231a3fff3fe887&#34;&gt;Removal&lt;/h4&gt;
</description>
    </item>
    
  </channel>
</rss>